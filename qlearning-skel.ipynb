{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning Example\n",
    "\n",
    "Andrew H. Fagg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym\n",
      "  Downloading gym-0.17.1.tar.gz (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 2.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy in /Users/qingn/anaconda3/lib/python3.7/site-packages (from gym) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /Users/qingn/anaconda3/lib/python3.7/site-packages (from gym) (1.16.4)\n",
      "Requirement already satisfied: six in /Users/qingn/anaconda3/lib/python3.7/site-packages (from gym) (1.12.0)\n",
      "Collecting pyglet<=1.5.0,>=1.4.0\n",
      "  Downloading pyglet-1.5.0-py2.py3-none-any.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 16.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /Users/qingn/anaconda3/lib/python3.7/site-packages (from gym) (1.2.1)\n",
      "Requirement already satisfied: future in /Users/qingn/anaconda3/lib/python3.7/site-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.17.1)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gym: filename=gym-0.17.1-py3-none-any.whl size=1648710 sha256=2223d6cab947e652c1d505c7fc760b81cbc2676d8f4be9ac3d10652ab837ee3b\n",
      "  Stored in directory: /Users/qingn/Library/Caches/pip/wheels/12/7a/2a/2e85bca5dd2c3b319675a5db8a48837b7cfe0603240442b771\n",
      "Successfully built gym\n",
      "Installing collected packages: pyglet, gym\n",
      "Successfully installed gym-0.17.1 pyglet-1.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import fnmatch\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "#from tensorflow import keras\n",
    "from tensorflow.keras.layers import LeakyReLU, UpSampling1D, Input, InputLayer, Reshape, Activation, Lambda, AveragePooling1D\n",
    "from tensorflow.keras.layers import Convolution2D, Dense, MaxPooling2D, Flatten, BatchNormalization, Dropout, Conv2DTranspose, Concatenate\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "import random\n",
    "#import skimage.transform as sktr\n",
    "import gym\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import re\n",
    " \n",
    "\n",
    "#from sklearn.p\n",
    "import sklearn.metrics\n",
    "\n",
    "from sklearn.utils.extmath import cartesian\n",
    "\n",
    "####################################\n",
    "\n",
    "FONTSIZE = 18\n",
    "FIGURE_SIZE = (10,4)\n",
    "FIGURE_SIZE2 = (10,10)\n",
    "\n",
    "# Configure parameters\n",
    "plt.rcParams.update({'font.size': FONTSIZE, 'figure.figsize': FIGURE_SIZE})\n",
    "\n",
    "# Default tick label size\n",
    "plt.rcParams['xtick.labelsize'] = FONTSIZE\n",
    "plt.rcParams['ytick.labelsize'] = FONTSIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class numpyBuffer:\n",
    "    '''\n",
    "    Circular buffer using a numpy array\n",
    "    \n",
    "    In this case, we only append to this buffer and overwrite values once we wrap-around\n",
    "    '''\n",
    "    def __init__(self, maxsize=100, ndims=1, dtype=np.float32):\n",
    "        '''\n",
    "        Constructor for the buffer\n",
    "        \n",
    "        :param maxsize: Maximum number of rows that can be stored in the buffer\n",
    "        :param ndims: The number of columns in the buffer       \n",
    "        '''\n",
    "        \n",
    "        self.buffer = np.zeros((maxsize,ndims), dtype=dtype)\n",
    "        self.maxsize=maxsize\n",
    "        self.ndims=ndims\n",
    "        self.back = 0\n",
    "        self.full = False\n",
    "    \n",
    "    def size(self):\n",
    "        '''\n",
    "        :return: The number of items stored in the buffer\n",
    "        '''\n",
    "        if(self.full):\n",
    "            return self.maxsize\n",
    "        else:\n",
    "            return self.back\n",
    "        \n",
    "    def append(self, rowvec):\n",
    "        '''\n",
    "        Append a row to the buffer\n",
    "        \n",
    "        :param rowvec: Numpy row vector of values to append.  Must be 1xndims\n",
    "        '''\n",
    "        self.buffer[self.back,:] = rowvec\n",
    "        self.back = self.back+1\n",
    "        if self.back >= self.maxsize:\n",
    "            self.back = 0\n",
    "            self.full = True\n",
    "            \n",
    "    def getrows(self, row_indices):\n",
    "        '''\n",
    "        Return a set of indicated rows\n",
    "        \n",
    "        :param row_indices: Array of row indices into the buffer\n",
    "        :return: len(row_indices)xndims numpy array\n",
    "        '''\n",
    "        return self.buffer[row_indices,:]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-2600f8d93c89>, line 200)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-2600f8d93c89>\"\u001b[0;36m, line \u001b[0;32m200\u001b[0m\n\u001b[0;31m    q_next = ????\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class myAgent:\n",
    "    def __init__(self, state_size, action_size, action_continuous, epsilon=.01, gamma=0.99, \n",
    "                 lrate=.001, action_discrete=True, maxlen=10000):\n",
    "        '''\n",
    "        :param state_size: Number of state variables\n",
    "        :param action_size: Number of actions (will use one-hot encoded actions)\n",
    "        :param action_continuous: List of continuous actions that correspond to the discrete choices\n",
    "        :param epsilon: Constant exploration rate\n",
    "        :param gamma: Constant discout rate\n",
    "        :param lrate: Learning rate\n",
    "        :param action_discrete: Network produces one Q-value for each discrete action \n",
    "                (True is the only supported case)\n",
    "        :param maxlen: Maximum length of the circular experience buffer\n",
    "        \n",
    "        Experience buffer is designed for quick access to prior experience\n",
    "        '''\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.action_continuous = action_continuous\n",
    "        self.epsilon=epsilon\n",
    "        self.gamma=gamma\n",
    "        self.reward_log = []\n",
    "        self.verbose = False\n",
    "        self.verbose_execute = False\n",
    "        self.lrate=lrate\n",
    "        self.action_discrete=action_discrete\n",
    "        self.log_observation = numpyBuffer(maxlen, state_size)\n",
    "        self.log_observation_new = numpyBuffer(maxlen, state_size)\n",
    "        self.log_action = numpyBuffer(maxlen, 1, dtype=np.int16)\n",
    "        self.log_reward = numpyBuffer(maxlen, 1)\n",
    "        self.log_done = numpyBuffer(maxlen, 1, dtype=np.bool)\n",
    "        \n",
    "        \n",
    "    def build_model(self, n_units, activation='elu', lambda_regularization=None):\n",
    "        '''\n",
    "        Simple sequential model.\n",
    "        \n",
    "        :param n_units: Number of units in each hidden layer (a list)\n",
    "        :param activation: Activation function for the hidden units\n",
    "        :param lambda_regularization: None or a continuous value (currently not used)\n",
    "        '''\n",
    "        model = Sequential()\n",
    "        self.model = model\n",
    "        i = 0\n",
    "        \n",
    "        # Input layer\n",
    "        model.add(InputLayer(input_shape=(self.state_size,)))\n",
    "        \n",
    "        # Loop over hidden layers\n",
    "        for n in n_units:\n",
    "            model.add(Dense(n, \n",
    "                        activation=activation,\n",
    "                        use_bias=True,\n",
    "                        kernel_initializer='truncated_normal', \n",
    "                        bias_initializer='zeros', \n",
    "                        name = \"D\"+str(i)))\n",
    "                        #kernel_regularizer=keras.regularizers.l2(lambda_regularization),\n",
    "                        #bias_regularizer=keras.regularizers.l2(lambda_regularization)))\n",
    "            i=i+1\n",
    "            \n",
    "        # model.add(BatchNormalization())\n",
    "        # Output layer\n",
    "        model.add(Dense(self.action_size, \n",
    "                        activation=None,\n",
    "                        use_bias=True,\n",
    "                        kernel_initializer='truncated_normal', \n",
    "                        bias_initializer='zeros',  \n",
    "                        name = \"D\"+str(i)))\n",
    "                        #kernel_regularizer=keras.regularizers.l2(lambda_regularization),\n",
    "                        #bias_regularizer=keras.regularizers.l2(lambda_regularization)))\n",
    "        \n",
    "        # Configure model\n",
    "        opt = keras.optimizers.Adam(lr=self.lrate, beta_1=0.9, beta_2=0.999, \n",
    "                            epsilon=None, decay=0.0, amsgrad=False)\n",
    "        \n",
    "        model.compile(loss='mse', optimizer=opt)\n",
    "        \n",
    "        print(model.summary())\n",
    "        \n",
    "    def build_model2(self, n_units, activation='elu', lambda_regularization=0.0):\n",
    "        '''\n",
    "        Model with an independent branch for each action.  (not used right now)\n",
    "        \n",
    "        Uses the Model API\n",
    "        \n",
    "        Regularization has been turned off\n",
    "        '''\n",
    "        \n",
    "        # Input layer\n",
    "        #model.add(InputLayer(input_shape=(self.state_size,)))\n",
    "        input_tensor = Input(shape=(self.state_size,), name='input')\n",
    "        \n",
    "        output_tensors = []\n",
    "        \n",
    "        # Loop over actions\n",
    "        for a in range(self.action_size):\n",
    "            \n",
    "            dense_tensor = input_tensor\n",
    "            # Loop over hidden layers\n",
    "            i = 0\n",
    "            for n in n_units:\n",
    "                dense_tensor = Dense(n, \n",
    "                        activation=activation,\n",
    "                        use_bias=True,\n",
    "                        kernel_initializer='truncated_normal', \n",
    "                        bias_initializer='zeros', \n",
    "                        name = \"D_Br\"+str(a)+\"_L\"+str(i))(dense_tensor)\n",
    "                        #kernel_regularizer=keras.regularizers.l2(lambda_regularization),\n",
    "                        #bias_regularizer=keras.regularizers.l2(lambda_regularization)))\n",
    "                i=i+1\n",
    "            \n",
    "            # Output layer\n",
    "            output_tensor = Dense(1, \n",
    "                        activation=None,\n",
    "                        use_bias=True,\n",
    "                        kernel_initializer='truncated_normal', \n",
    "                        bias_initializer='zeros', \n",
    "                        name = \"O\"+str(a))(dense_tensor)\n",
    "            \n",
    "                        #kernel_regularizer=keras.regularizers.l2(lambda_regularization),\n",
    "                        #bias_regularizer=keras.regularizers.l2(lambda_regularization)))\n",
    "            output_tensors.append(output_tensor)\n",
    "        \n",
    "        # Concatentate the actions together\n",
    "        output_tensor = Concatenate()(output_tensors)\n",
    "        \n",
    "        # Configure model\n",
    "        opt = keras.optimizers.Adam(lr=self.lrate, beta_1=0.9, beta_2=0.999, \n",
    "                            epsilon=None, decay=0.0, amsgrad=False)\n",
    "        \n",
    "        model = Model(inputs=input_tensor, outputs=output_tensor)\n",
    "        model.compile(loss='mse', optimizer=opt)\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "        print(model.summary())\n",
    "        \n",
    "        \n",
    "    def choose_action(self, observation, verbose=False):\n",
    "        '''\n",
    "        epsilon-greedy choice of discrete action\n",
    "        \n",
    "        :returns: (discrete_action, explore_bit)\n",
    "\n",
    "        '''\n",
    "        if(np.random.rand() <= self.epsilon):\n",
    "            return np.random.randint(self.action_size), True\n",
    "        else:\n",
    "            pred = self.model.predict(observation)[0]\n",
    "            if verbose:\n",
    "                print(pred)\n",
    "            return np.argmax(pred), False\n",
    "    \n",
    "    def choose_action_continuous(self, observation, verbose=False):\n",
    "        '''\n",
    "        epsilon-greedy choice of continuous action\n",
    "        \n",
    "        :returns: (discrete_action, continuous_action, explore_bit)\n",
    "        '''\n",
    "        observation = np.array(observation, ndmin=2)\n",
    "        action_index, explore = self.choose_action(observation, verbose)\n",
    "        return action_index, self.action_continuous[action_index], explore\n",
    "    \n",
    "    def log_experience(self, observation, action_index, reward, observation_new, done):\n",
    "        ''' \n",
    "        Store the last step in the circular buffer\n",
    "        '''\n",
    "        # Convert to numpy arrays\n",
    "        observation =  np.array(observation, ndmin=2)\n",
    "        observation_new =  np.array(observation_new, ndmin=2)\n",
    "        \n",
    "        self.log_observation.append(observation)\n",
    "        self.log_observation_new.append(observation_new)\n",
    "        self.log_action.append(action_index)\n",
    "        self.log_reward.append(reward)\n",
    "        self.log_done.append(done)\n",
    "                \n",
    "    def learning_step(self, batch_size=200):\n",
    "        '''\n",
    "        Iterate over a minibatch of the stored experience & take a learning step with each\n",
    "\n",
    "        :param batch_size: Size of the batch to do learning with\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        # Sample from the prior experience.  How we do this depends on how much\n",
    "        #  experience that we have accumulated so far\n",
    "        if self.log_observation.size() < batch_size:\n",
    "            minibatch_inds = range(self.log_observation.size())\n",
    "            #return\n",
    "        else:\n",
    "            # Random sample from the buffer\n",
    "            minibatch_inds = random.sample(range(self.log_observation.size()), batch_size)\n",
    "        \n",
    "        print(\"Creating batch:\", len(minibatch_inds))\n",
    "        observations = self.log_observation.getrows(minibatch_inds)\n",
    "        targets = self.model.predict(observations)\n",
    "        observations_new = self.log_observation_new.getrows(minibatch_inds)\n",
    "        \n",
    "        q_next = ????\n",
    "        q_next_max = ????\n",
    "        \n",
    "        rewards = self.log_reward.getrows(minibatch_inds)[:,0]\n",
    "\n",
    "        dones = self.log_done.getrows(minibatch_inds)[:,0]  \n",
    "        done_list = ????\n",
    "        done_not_list = ????\n",
    "        \n",
    "        actions = self.log_action.getrows(minibatch_inds)[:,0]\n",
    "        \n",
    "        # Update targets: for each example, only one action is updated\n",
    "        #  (the one that was actually executed)\n",
    "        \n",
    "        # Last step in the episodes\n",
    "        targets[done_list, actions[done_list]] = ????\n",
    "        # Other steps\n",
    "        targets[done_not_list, actions[done_not_list]] = ????\n",
    "        \n",
    "        # Update the Q-function\n",
    "        self.model.fit(observations, targets, epochs=1, verbose=0)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(observations, targets)\n",
    "    \n",
    "    def execute_trial(self, env, nsteps, render_flag=False, batch_size=100):\n",
    "        '''\n",
    "        A trial terminates at nsteps or when the environment says we must stop.\n",
    "        \n",
    "        '''\n",
    "        observation = env.reset()\n",
    "        \n",
    "        # Accumulator for total reward\n",
    "        reward_total = 0\n",
    "        \n",
    "        # Loop over each step\n",
    "        for i in range(nsteps):\n",
    "            if render_flag:\n",
    "                env.render()\n",
    "            \n",
    "            # Figure out which action to execute\n",
    "            action_index, action_continuous, explore = self.choose_action_continuous(observation, verbose=self.verbose_execute)\n",
    "            \n",
    "            # Some environments require discrete actions, while others require continous actions\n",
    "            if self.action_discrete:\n",
    "                observation_new, reward, done, info = env.step(action_index) #env.step(action_continuous)\n",
    "            else:\n",
    "                observation_new, reward, done, info = env.step(action_continuous)\n",
    "                \n",
    "            # Remember reward\n",
    "            reward_total = reward_total + reward\n",
    "            if self.verbose_execute:\n",
    "                print(observation, action_index, reward, observation_new, done)\n",
    "                \n",
    "            # Log this step \n",
    "            self.log_experience(observation, action_index, reward, \n",
    "                                    observation_new, done)\n",
    "                \n",
    "            if done:\n",
    "                # Environment says we are done\n",
    "                break\n",
    "                \n",
    "            # Prepare for the next step\n",
    "            observation = observation_new\n",
    "            \n",
    "        # Learning\n",
    "        #print(\"before learning\")\n",
    "        self.learning_step(batch_size=batch_size)\n",
    "        if render_flag:\n",
    "            env.close()\n",
    "        print(reward_total)\n",
    "        \n",
    "        # Log accumulated reward for this trial\n",
    "        self.reward_log.append(reward_total)\n",
    "        \n",
    "    def execute_ntrials(self, env, ntrials, nsteps, render_flag=False, batch_size=100):\n",
    "        '''\n",
    "        Execute the specified number of trials\n",
    "        '''\n",
    "        for _ in range(ntrials):\n",
    "            self.execute_trial(env, nsteps, render_flag, batch_size)\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cart-Pole example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'myAgent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-4100da30750c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Cart-pole is a discrete action environment (provided continous values are dummies)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlrate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'myAgent' is not defined"
     ]
    }
   ],
   "source": [
    "# Cart-pole is a discrete action environment (provided continous values are dummies)\n",
    "agent = myAgent(4, 2, [[-1], [1]], gamma=0.99, epsilon=0.1, lrate=.001)\n",
    "agent.build_model([20, 10, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent.execute_ntrials(env, 100,1000,render_flag=False, batch-size = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "agent.verbose_execute = Tr\n",
    "agent.verbose = Tr\n",
    ".excute_trial(env,100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show accumulated reward as a function of trial\n",
    "plt.plot(agent.reward_log)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pendulum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env2 = gym.make('Pendulum-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show accumulated reward as a function of trial\n",
    "plt.plot(agent2.reward_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn while rendering\n",
    "agent2.execute_ntrials(env2, 10, 1000, render_flag=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
